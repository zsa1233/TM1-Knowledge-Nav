# Parameter Efficient Fine Tuning using Transformers and LoRA
Learn how to fine tune a LoRA. 

## See it live and in action ğŸ“º - Click the image!
<a href="https://youtu.be/D3pXSkGceY0"><img src="https://i.imgur.com/nEfrhIQ.png"/></a>

# Startup ğŸš€
1. Startup on your machine locally by running `uv init`
2. Install all the stuff `uv sync`
3. Create samples `uv run syntheticdatageneration.py`
4. Process them `uv run preprocessing.py`
5. Improve quality `uv run dataquality`
6. Transition to RunPod and install uv `pip install uv`
7. Create uv project on RunPod `uv init` 
8. Upload server toml file to runpod `pyproject_use_this_one_on_runpod.toml` (rename it to pyproject.toml once on the server) and sync `uv sync`
9. Copy the data folder and the instructionquality file and train `uv run train.py`
10. Deploy using Ollama and Langflow - shown in vid!

# Who, When, Why?

ğŸ‘¨ğŸ¾â€ğŸ’» Author: Nick Renotte <br />
ğŸ“… Version: 1.x<br />
ğŸ“œ License: This project is licensed under the MIT License </br>
